{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejwC1rc9Y1Qo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "df = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Remove URL\n",
        "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
        "subs_url = r'url-web'\n",
        "df['message'] = df['message'].replace(to_replace=pattern_url, value=subs_url, regex=True)\n",
        "\n",
        "# Remove punctuation and convert to lowercase\n",
        "df['message'] = df['message'].str.lower().apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "# Tokenize Data\n",
        "tokeniser = TreebankWordTokenizer()\n",
        "df['message'] = df['message'].apply(tokeniser.tokenize)\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "df['message'] = df['message'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['message'] = df['message'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "# Remove Stopwords\n",
        "def remove_stop_words(tokens):\n",
        "    return [t for t in tokens if t not in stop_words]\n",
        "\n",
        "df['message'] = df['message'].apply(remove_stop_words)\n",
        "\n",
        "# Feature Selection Splitting out Variables\n",
        "y = df['sentiment']\n",
        "X = df['message']\n",
        "\n",
        "bow_vect = CountVectorizer(max_features=1000, stop_words='english')\n",
        "X_vect = bow_vect.fit_transform(df['message'])\n",
        "\n",
        "trans = StandardScaler(with_mean=False)\n",
        "X_vect = trans.fit_transform(X_vect)\n",
        "\n",
        "# Training Model\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_vect, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model training\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(X_train, y_train)\n",
        "rfc_pred = rfc.predict(X_val)\n",
        "\n",
        "# Test Set\n",
        "testx = test['message']\n",
        "test_vect = bow_vect.transform(testx)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rfc.predict(test_vect)\n",
        "\n",
        "# Test\n",
        "test['sentiment'] = y_pred\n",
        "test[['tweetid', 'sentiment']].to_csv('testsubmission.csv', index=False)"
      ]
    }
  ]
}